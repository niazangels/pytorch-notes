{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pack padded / pad packed\n",
    "Source: https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import LSTM, Embedding\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to run LSTM on a batch of 3 character sequences `['long_str', 'tiny', 'medium']`\n",
    "\n",
    " *  Step 1: Construct Vocabulary\n",
    " *  Step 2: Load indexed data (list of instances, where each instance is list of character indices)\n",
    " *  Step 3: Make Model\n",
    " *  Step 4: Pad instances with 0s till max length sequence\n",
    " *  Step 5: Sort instances by sequence length in descending order\n",
    " *  Step 6: Embed the instances\n",
    " *  Step 7: Call pack_padded_sequence with embeded instances and sequence lengths\n",
    " *  Step 8: Forward with LSTM\n",
    " *  Step 9: Call unpack_padded_sequences if required / or just pick last hidden vector\n",
    " *  Summary of Shape Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = ['long_str',  # len = 8\n",
    "        'tiny',      # len = 4\n",
    "        'medium']    # len = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '_', 'd', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'y']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Construct Vocabulary\n",
    "vocab = ['<pad>'] + sorted(list(set([x for word in seqs for x in word])))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 9, 8, 4, 1, 11, 12, 10], [12, 5, 8, 14], [7, 3, 2, 5, 13, 7]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load indexed data (list of instances, where each instance is list of character indices)\n",
    "vectorized_seqs = [[vocab.index(x) for x in word] for word in seqs]\n",
    "vectorized_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Make Model\n",
    "embed = Embedding(len(vocab), 4) # embedding size = 4\n",
    "lstm = LSTM(input_size=4, hidden_size=5, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Pad instances with 0s till max length sequence\n",
    "seq_lengths = torch.LongTensor([len(x) for x in vectorized_seqs])\n",
    "\n",
    "# seq_lengths => [ 8, 4,  6]\n",
    "# batch_sum_seq_len: 8 + 4 + 6 = 18\n",
    "# max_seq_len: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor = torch.zeros(len(vectorized_seqs), max(seq_lengths)).long()\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(vectorized_seqs):\n",
    "    seq_tensor[i, :len(x)] = torch.LongTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  9,  8,  4,  1, 11, 12, 10],\n",
       "        [12,  5,  8, 14,  0,  0,  0,  0],\n",
       "        [ 7,  3,  2,  5, 13,  7,  0,  0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Step 5: Sort instances by sequence length in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths, idxs = seq_lengths.sort(0, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  9,  8,  4,  1, 11, 12, 10],\n",
       "        [ 7,  3,  2,  5, 13,  7,  0,  0],\n",
       "        [12,  5,  8, 14,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor = seq_tensor[idxs]\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.9409e-01, -2.8896e+00,  6.9483e-01, -1.2146e+00],\n",
       "         [ 1.9987e+00,  6.2710e-02, -2.8276e-01,  1.5810e+00],\n",
       "         [-2.5376e-01, -1.0364e+00, -4.3581e-01, -8.3744e-01],\n",
       "         [ 9.3385e-01,  2.1982e-01,  1.0951e-01, -6.4361e-01],\n",
       "         [-1.1321e+00,  1.6221e+00, -4.6939e-01,  8.1539e-01],\n",
       "         [ 7.4638e-01, -6.3624e-01,  1.0528e-02, -7.6883e-02],\n",
       "         [-5.5516e-01, -1.2022e+00,  1.1431e+00, -7.5028e-01],\n",
       "         [-5.3181e-01,  1.9000e+00,  5.9851e-01,  6.9586e-01]],\n",
       "\n",
       "        [[-1.4821e+00, -7.2892e-01,  3.9801e-01, -3.1882e-01],\n",
       "         [ 5.2833e-01, -2.7108e+00,  6.2506e-01,  7.2514e-01],\n",
       "         [ 5.2835e-01,  1.5062e+00,  9.0398e-01,  2.5230e-03],\n",
       "         [-5.9331e-01, -1.1998e+00, -1.2029e+00, -8.4205e-01],\n",
       "         [-3.8441e-01, -8.1292e-01, -3.6298e-01, -1.1046e+00],\n",
       "         [-1.4821e+00, -7.2892e-01,  3.9801e-01, -3.1882e-01],\n",
       "         [-1.6660e-01,  1.8722e+00, -2.9307e+00, -4.1734e-01],\n",
       "         [-1.6660e-01,  1.8722e+00, -2.9307e+00, -4.1734e-01]],\n",
       "\n",
       "        [[-5.5516e-01, -1.2022e+00,  1.1431e+00, -7.5028e-01],\n",
       "         [-5.9331e-01, -1.1998e+00, -1.2029e+00, -8.4205e-01],\n",
       "         [-2.5376e-01, -1.0364e+00, -4.3581e-01, -8.3744e-01],\n",
       "         [ 1.6695e+00, -1.8800e+00,  1.3798e+00, -8.4393e-01],\n",
       "         [-1.6660e-01,  1.8722e+00, -2.9307e+00, -4.1734e-01],\n",
       "         [-1.6660e-01,  1.8722e+00, -2.9307e+00, -4.1734e-01],\n",
       "         [-1.6660e-01,  1.8722e+00, -2.9307e+00, -4.1734e-01],\n",
       "         [-1.6660e-01,  1.8722e+00, -2.9307e+00, -4.1734e-01]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Embed the instances\n",
    "embedded_seq_tensor = embed(seq_tensor)\n",
    "embedded_seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedded_seq_tensor =>\n",
    "#                       [[[-0.77578706 -1.8080667  -1.1168439   1.1059115 ]     l\n",
    "#                         [-0.23622951  2.0361056   0.15435742 -0.04513785]     o\n",
    "#                         [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     n\n",
    "#                         [ 0.40524676  0.98665565 -0.08621677 -1.1728264 ]     g\n",
    "#                         [-1.6334635  -0.6100042   1.7509955  -1.931793  ]     _\n",
    "#                         [-0.6470658  -0.6266589  -1.7463604   1.2675372 ]     s\n",
    "#                         [ 0.64004815  0.45813003  0.3476034  -0.03451729]     t\n",
    "#                         [-0.22739866 -0.45782727 -0.6643252   0.25129375]]    r\n",
    "\n",
    "#                        [[ 0.16031227 -0.08209462 -0.16297023  0.48121014]     m\n",
    "#                         [-0.7303265  -0.857339    0.58913064 -1.1068314 ]     e\n",
    "#                         [ 0.48159844 -1.4886451   0.92639893  0.76906884]     d\n",
    "#                         [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     i\n",
    "#                         [ 0.01795524 -0.59048957 -0.53800726 -0.6611691 ]     u\n",
    "#                         [ 0.16031227 -0.08209462 -0.16297023  0.48121014]     m\n",
    "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     <pad>\n",
    "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]]    <pad>\n",
    "\n",
    "#                        [[ 0.64004815  0.45813003  0.3476034  -0.03451729]     t\n",
    "#                         [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     i\n",
    "#                         [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     n\n",
    "#                         [-1.284392    0.68294704  1.4064184  -0.42879772]     y\n",
    "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     <pad>\n",
    "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     <pad>\n",
    "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     <pad>\n",
    "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]]]   <pad>\n",
    "# embedded_seq_tensor.shape : (batch_size X max_seq_len X embedding_dim) = (3 X 8 X 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
    "# packed_input (PackedSequence is NamedTuple with 2 attributes: data and batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packed_input.data =>\n",
    "#                         [[-0.77578706 -1.8080667  -1.1168439   1.1059115 ]     l\n",
    "#                          [ 0.01795524 -0.59048957 -0.53800726 -0.6611691 ]     m\n",
    "#                          [-0.6470658  -0.6266589  -1.7463604   1.2675372 ]     t\n",
    "#                          [ 0.16031227 -0.08209462 -0.16297023  0.48121014]     o\n",
    "#                          [ 0.40524676  0.98665565 -0.08621677 -1.1728264 ]     e\n",
    "#                          [-1.284392    0.68294704  1.4064184  -0.42879772]     i\n",
    "#                          [ 0.64004815  0.45813003  0.3476034  -0.03451729]     n\n",
    "#                          [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     d\n",
    "#                          [ 0.64004815  0.45813003  0.3476034  -0.03451729]     n\n",
    "#                          [-0.23622951  2.0361056   0.15435742 -0.04513785]     g\n",
    "#                          [ 0.16031227 -0.08209462 -0.16297023  0.48121014]     i\n",
    "#                          [-0.22739866 -0.45782727 -0.6643252   0.25129375]]    y\n",
    "#                          [-0.7303265  -0.857339    0.58913064 -1.1068314 ]     _\n",
    "#                          [-1.6334635  -0.6100042   1.7509955  -1.931793  ]     u\n",
    "#                          [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     s\n",
    "#                          [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     m\n",
    "#                          [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     t\n",
    "#                          [ 0.48159844 -1.4886451   0.92639893  0.76906884]     r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_input.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3, 3, 2, 2, 1, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_input.batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l  o  n  g  _  s  t  r   #(long_str)\n",
    "# m  e  d  i  u  m         #(medium)\n",
    "# t  i  n  y               #(tiny)\n",
    "# 3  3  3  3  2  2  1  1   (sum = 18 [batch_sum_seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Forward with LSTM\n",
    "packed_output, (ht,ct) = lstm(packed_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packed_output (PackedSequence is NamedTuple with 2 attributes: data and batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packed_output.data :\n",
    "#                          [[-0.00947162  0.07743231  0.20343193  0.29611713  0.07992904]   l\n",
    "#                           [ 0.08596145  0.09205993  0.20892891  0.21788561  0.00624391]   o\n",
    "#                           [ 0.16861682  0.07807446  0.18812777 -0.01148055 -0.01091915]   n\n",
    "#                           [ 0.20994528  0.17932937  0.17748171  0.05025435  0.15717036]   g\n",
    "#                           [ 0.01364102  0.11060348  0.14704391  0.24145307  0.12879576]   _\n",
    "#                           [ 0.02610307  0.00965587  0.31438383  0.246354    0.08276576]   s\n",
    "#                           [ 0.09527554  0.14521319  0.1923058  -0.05925677  0.18633027]   t\n",
    "#                           [ 0.09872741  0.13324396  0.19446367  0.4307988  -0.05149471]   r\n",
    "#                           [ 0.03895474  0.08449443  0.18839942  0.02205326  0.23149511]   m\n",
    "#                           [ 0.14620507  0.07822411  0.2849248  -0.22616537  0.15480657]   e\n",
    "#                           [ 0.00884941  0.05762182  0.30557525  0.373712    0.08834908]   d\n",
    "#                           [ 0.12460691  0.21189159  0.04823487  0.06384943  0.28563985]   i\n",
    "#                           [ 0.01368293  0.15872964  0.03759198 -0.13403234  0.23890573]   u\n",
    "#                           [ 0.00377969  0.05943518  0.2961751   0.35107893  0.15148178]   m\n",
    "#                           [ 0.00737647  0.17101538  0.28344846  0.18878219  0.20339936]   t\n",
    "#                           [ 0.0864429   0.11173367  0.3158251   0.37537992  0.11876849]   i\n",
    "#                           [ 0.17885767  0.12713005  0.28287745  0.05562563  0.10871304]   n\n",
    "#                           [ 0.09486895  0.12772645  0.34048414  0.25930756  0.12044918]]  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 5])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_output.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3, 3, 2, 2, 1, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_output.batch_sizes # same as packed_input.batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization :\n",
    "\n",
    "# l  o  n  g  _  s  t  r   #(long_str)\n",
    "# m  e  d  i  u  m         #(medium)\n",
    "# t  i  n  y               #(tiny)\n",
    "# 3  3  3  3  2  2  1  1   (sum = 18 [batch_sum_seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Call unpack_padded_sequences if required / or just pick last hidden vector\n",
    "output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output =>\n",
    "#                          [[[-0.00947162  0.07743231  0.20343193  0.29611713  0.07992904]   l\n",
    "#                            [ 0.20994528  0.17932937  0.17748171  0.05025435  0.15717036]   o\n",
    "#                            [ 0.09527554  0.14521319  0.1923058  -0.05925677  0.18633027]   n\n",
    "#                            [ 0.14620507  0.07822411  0.2849248  -0.22616537  0.15480657]   g\n",
    "#                            [ 0.01368293  0.15872964  0.03759198 -0.13403234  0.23890573]   _\n",
    "#                            [ 0.00737647  0.17101538  0.28344846  0.18878219  0.20339936]   s\n",
    "#                            [ 0.17885767  0.12713005  0.28287745  0.05562563  0.10871304]   t\n",
    "#                            [ 0.09486895  0.12772645  0.34048414  0.25930756  0.12044918]]  r\n",
    "\n",
    "#                           [[ 0.08596145  0.09205993  0.20892891  0.21788561  0.00624391]   m\n",
    "#                            [ 0.01364102  0.11060348  0.14704391  0.24145307  0.12879576]   e\n",
    "#                            [ 0.09872741  0.13324396  0.19446367  0.4307988  -0.05149471]   d\n",
    "#                            [ 0.00884941  0.05762182  0.30557525  0.373712    0.08834908]   i\n",
    "#                            [ 0.00377969  0.05943518  0.2961751   0.35107893  0.15148178]   u\n",
    "#                            [ 0.0864429   0.11173367  0.3158251   0.37537992  0.11876849]   m\n",
    "#                            [ 0.          0.          0.          0.          0.        ]   <pad>\n",
    "#                            [ 0.          0.          0.          0.          0.        ]]  <pad>\n",
    "\n",
    "#                           [[ 0.16861682  0.07807446  0.18812777 -0.01148055 -0.01091915]   t\n",
    "#                            [ 0.02610307  0.00965587  0.31438383  0.246354    0.08276576]   i\n",
    "#                            [ 0.03895474  0.08449443  0.18839942  0.02205326  0.23149511]   n\n",
    "#                            [ 0.12460691  0.21189159  0.04823487  0.06384943  0.28563985]   y\n",
    "#                            [ 0.          0.          0.          0.          0.        ]   <pad>\n",
    "#                            [ 0.          0.          0.          0.          0.        ]   <pad>\n",
    "#                            [ 0.          0.          0.          0.          0.        ]   <pad>\n",
    "#                            [ 0.          0.          0.          0.          0.        ]]] <pad>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 5])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape # ( batch_size X max_seq_len X hidden_dim) = (3 X 8 X 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0044, -0.2581,  0.1075, -0.0793,  0.2602],\n",
      "        [ 0.1094,  0.0606,  0.0343, -0.0094,  0.3211],\n",
      "        [ 0.0820,  0.1148, -0.0205,  0.1659,  0.3319]],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Or if you just want the final hidden state?\n",
    "print(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary of Shape Transformations ##\n",
    "##----------------------------------##\n",
    "\n",
    "# (batch_size X max_seq_len X embedding_dim) --> Sort by seqlen ---> (batch_size X max_seq_len X embedding_dim)\n",
    "# (batch_size X max_seq_len X embedding_dim) --->      Pack     ---> (batch_sum_seq_len X embedding_dim)\n",
    "# (batch_sum_seq_len X embedding_dim)        --->      LSTM     ---> (batch_sum_seq_len X hidden_dim)\n",
    "# (batch_sum_seq_len X hidden_dim)           --->    UnPack     ---> (batch_size X max_seq_len X hidden_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
